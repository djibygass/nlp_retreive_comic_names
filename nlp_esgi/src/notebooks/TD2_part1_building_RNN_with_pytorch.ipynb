{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "895a119a",
   "metadata": {},
   "source": [
    "# Building RNN with Pytorch\n",
    "\n",
    "Dans ce TD, nous allons construire un RNN from scratch avec Pytorch, en suivant les étapes suivantes:\n",
    "1. Sur un problème linéaire, nous allons utiliser les torch optimizers pour trouver le paramètre de régression idéal\n",
    "2. Sur un problème non-linéaire, nous allons construire un multi-layers perceptron\n",
    "3. Sur un problème de texte, nous allons construire un modèle RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9929e27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511cfa9c",
   "metadata": {},
   "source": [
    "## Optimizers\n",
    "\n",
    "### Données example\n",
    "Nous allons construire des données $y = \\beta^* X + b$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d004af",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "\n",
    "a_star = 3.\n",
    "b_star = -3.\n",
    "noise_std = 1\n",
    "\n",
    "x = (np.random.rand(n, 1) - 0.5) * 4\n",
    "noise = np.random.normal(0, noise_std, (n, 1))\n",
    "y = a_star * x + b_star + noise\n",
    "\n",
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "xvec = x\n",
    "plt.plot(x, y, 'o', markersize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30c3616",
   "metadata": {},
   "source": [
    "### Constuire l'optimizer\n",
    "\n",
    "- Avec torch.nn.Parameter(), créer le paramètre beta qui sera optimisé. Les predictions du modèle sont égales à $\\beta \\times X$ \n",
    "- Avec torch.nn.MSELoss, déclarer la loss entre les prédictions et le résultat réel. loss.backward() pour mettre à jour les gradients\n",
    "- Avec torch.optim.Adam, déclarer un optimizer\n",
    "- Construisez la boucle qui, pour n_epochs, va reset les gradients, calculer la loss, mettre à jour les gradients et faire un pas pour optimiser beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5af62a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b669ad46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot des predictions\n",
    "plt.plot(x, y, 'o', markersize=10)\n",
    "plt.plot(x, beta * x, 'rx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7664db",
   "metadata": {},
   "source": [
    "## Construire un multi-layer perceptron (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1270600a",
   "metadata": {},
   "source": [
    "Ici, les données sont $y = a x^2 + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad053927",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "\n",
    "a_star = 3.\n",
    "b_star = -3.\n",
    "noise_std = 1\n",
    "\n",
    "x = (np.random.rand(n, 1) - 0.5) * 4\n",
    "noise = np.random.normal(0, noise_std, (n, 1))\n",
    "y = a_star * (x ** 2) + b_star + noise\n",
    "\n",
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "xvec = x\n",
    "plt.plot(x, y, 'o', markersize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d285840",
   "metadata": {},
   "source": [
    "Nous allons construire un Pytorch modèle. Dans ce framework, on définit la fonction \"forward\" qui prend en argument les inputs et retourne les prédictions.\n",
    "\n",
    "A l'aide de torch.nn.Module et de différentes layers (torch.nn.Linear, torch.nn.ReLU), terminez la classe ci-dessous pour créer un multi-layers perceptron "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fe1d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        prev_size = input_size\n",
    "            \n",
    "        self.layer = torch.nn.Linear(input_size, 1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.layer(X)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6678cd7",
   "metadata": {},
   "source": [
    "En utilisant le travail précédent, faite une cellule utilisant un torch optimizer pour optimiser le MLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80e23d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b056a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot des predictions\n",
    "plt.plot(x, y, 'o', markersize=10)\n",
    "plt.plot(x, model(x), 'rx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b679a74",
   "metadata": {},
   "source": [
    "## Construire un Recurrent Neural Network (RNN) from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69393034",
   "metadata": {},
   "source": [
    "### Données\n",
    " \n",
    "Téléchargez [ici](https://download.pytorch.org/tutorial/data.zip) un jeu de données \"nom\" -> pays d'origine du nom.\n",
    "Les noms de chaque pays sont dans leur fichier texte \"[country].txt\"\n",
    "\n",
    "A partir de ces données, créez X la liste des noms et y le pays d'origine de chaque nom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e33ad1",
   "metadata": {},
   "source": [
    "### Aide pour générer les données\n",
    "\n",
    "A la fin de ce code, on lit les noms et leur pays d'origine.<br>\n",
    "On a une fonction randomTrainingExample, qui génère le pays d'origine et le nom (en texte), puis l'ID de ce pays et le tenseur représetant le nom. <br>\n",
    "\n",
    "A partir de là, vous pouvez générer les données X, y comme vous voulez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8669e66d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Code pour générer le jeu de données\n",
    "# En admettant que les fichiers ont été extraits dans data/raw/names/\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import random\n",
    "import string\n",
    "import time\n",
    "import unicodedata\n",
    "\n",
    "path = Path(\"../data/raw/names/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed6fb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_names(path):\n",
    "    category_lines = {}\n",
    "    for file in path.iterdir():\n",
    "        if not file.name.endswith(\".txt\"):\n",
    "            continue\n",
    "            \n",
    "        with open(file) as f:\n",
    "            names = [unicodeToAscii(line.strip()) for line in f]\n",
    "        \n",
    "        category = file.stem\n",
    "        category_lines[category] = names\n",
    "\n",
    "    return category_lines\n",
    "\n",
    "# Handling ASCII stuff\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc37069",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_lines = read_names(path)\n",
    "all_categories = list(category_lines.keys())\n",
    "n_categories = len(all_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312870b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Find letter index from all_letters, e.g. \"a\" = 0\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def lineToTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b606108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomChoice(l):\n",
    "    return l[random.randint(0, len(l) - 1)]\n",
    "\n",
    "def randomTrainingExample():\n",
    "    category = randomChoice(all_categories)\n",
    "    line = randomChoice(category_lines[category])\n",
    "    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
    "    line_tensor = lineToTensor(line)\n",
    "    return category, line, category_tensor, line_tensor\n",
    "\n",
    "for i in range(10):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    print('category =', category, '/ line =', line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199a9224",
   "metadata": {},
   "source": [
    "### Construire le RNN\n",
    "\n",
    "Un Recurrent Neural Network prend, pour prédire un input X et un hidden state H.<br>\n",
    "Il output une prédiction et un nouvel hidden state pour la prochaine étape.<br>\n",
    "Nous allons construire un modèle:\n",
    "\n",
    "$f(name, H_0) = (proba(\\mbox{name from country 0}), proba(\\mbox{name from country 1}), \\dots)$\n",
    "\n",
    "Avec $H_0$ un hidden state initial (qui sera appris).\n",
    "\n",
    "Pour l'instant, nous allons construire un RNN linéaire.<br>\n",
    "Comme pour le MLP, construisez un torch.nn.Module:\n",
    "- A l'init, la classe prendra les arguments input_size, hidden_state_dim, output_size\n",
    "- L'init déclare un paramètre pour l'hidden state initial $H_0$, une layer linéaire prenant (input, hidden_state) -> output_size\n",
    "- Le forward fonctionnera ainsi:\n",
    "    - hidden_state = $H_0$\n",
    "    - For letter in name:\n",
    "        - x = one-hot encode (letter)\n",
    "        - output, hidden_state = linear_layer(x, hidden_state)\n",
    "    - return softmax(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab32234",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61972503",
   "metadata": {},
   "source": [
    "A l'aide de la negative log likelihood (torch.nn.NLLLoss), construisez une boucle optimisant ce modèle (optimizer, itération au sein des examples).\n",
    "\n",
    "**Attention 1:** Ici, on n'a pas des tenseurs X, y. On va prendre les exemples 1 par 1.<br>\n",
    "**Attention 2:** Si le modèle apprend les examples dans l'ordre (d'abord tous les noms arabes, puis tous les chinois, puis tous les tchèques, etc), cela changera grandement le résulat (et le modèle sera très mauvais)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce66341",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2625860",
   "metadata": {},
   "source": [
    "## Utilisez les RNNs de Pytorch\n",
    "\n",
    "Sur le même jeu de données, nous allons utiliser les RNNs déjà codés par Pytorch.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264251cf",
   "metadata": {},
   "source": [
    "### Padding\n",
    "\n",
    "Nous voulons aussi utiliser des tenseurs X et y pour utiliser data loader pour les batchs, etc\n",
    "\n",
    "\n",
    "Avec du padding, créer un tenseur X de la taille (nb_data, name_max_length, vocab_size). <br>\n",
    "X[i, j, :] contient le one-hot encode de la j-ème lettre du i-ème prénom. Si le i-ème prénom a moins de j lettre, alors X[i, j, :] est un vecteur nul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa21023",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0e91689",
   "metadata": {},
   "source": [
    "### Modèle\n",
    "\n",
    "Construisez un modèle qui:\n",
    "- A l'init, déclare un hidden state initial $H_0$, un RNN (avec torch.nn.RNN)\n",
    "- Au forward, passe (input, $H_0$) dans le RNN, puis applique un softmax à l'output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b60d00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a81d080",
   "metadata": {},
   "source": [
    "Entraînez le modèle avec des batchs de X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c675881",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eda0372a",
   "metadata": {},
   "source": [
    "### Amélioration du modèle\n",
    "\n",
    "Tenter d'améliorer le modèle:\n",
    "- En utilisant plusieurs layers dans le RNN\n",
    "- En utilisant LSTM à la place de RNN (une autre forme de recurrent neural network, plus avancé)\n",
    "\n",
    "(regardez la doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f688924b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da84e069",
   "metadata": {},
   "source": [
    "### Embedding\n",
    "\n",
    "On veut utiliser en embedding pour les lettres.\n",
    "Le forward appliquerait l'embedding aux lettres, puis les layers RNN prendrait l'embedding comme input\n",
    "\n",
    "Nous allons utiliser torch.nn.Embedding\n",
    "**Attention**: Embedding ne prend pas les lettres one-hot encoded, mais leurs indices.\n",
    "Par exemple, \"abca\", il lui faudrait le vecteur [0, 1, 2, 0]<br>\n",
    "et non [[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0418088",
   "metadata": {},
   "source": [
    "**Expérimenter torch.nn.Embedding**<br>\n",
    "Instantier \"emb\" une torch.nn.Embedding avec les bons paramètres (regardez la doc)\n",
    "En construisant le bon tenseur X_emb,\n",
    "\n",
    "```emb(X_emb)```\n",
    "\n",
    "doit retourner un tenseur (qui n'a pas de sens car il n'a rien appris pour l'instant, mais les opérations se passent bien)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617eb47b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "508cc851",
   "metadata": {},
   "source": [
    "Construisez un modèle RNN, où les lettres passent par l'embedding avant d'être envoyées dans les couches de RNN.\n",
    "Construisez une loop d'apprentissage\n",
    "\n",
    "Expérimentez plusieurs valeurs pour le nombre de couche, la taille d'embedding ou les tailles d'hidden state.<br>\n",
    "Quel neg log likelihood obtenez-vous ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e58d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
